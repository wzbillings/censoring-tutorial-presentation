{
  "hash": "a0bdb9f384aaea18e809c58508812cd7",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear models and censored data\"\nauthor: \"Zane Billings\"\ndate: last-modified\ndate-format: iso\nformat:\n  revealjs:\n    scrollable: true\n    auto-stretch: false\n    smaller: true\n    slide-number: true\nexecute: \n  freeze: auto\n  echo: true\n---\n\n\n\n\n\n\n## Censored data are partially known.\n\n### Survival outcomes are almost always censored.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/right censored example figure-1.png){width=960}\n:::\n:::\n\n\n\n\n* Participants 2, 6, and 10 have event times in $\\left[\\text{May 16}, \\infty\\right).$\n* If we just throw those people out, or count their event date as May 16, we\nbias the distribution of event times to be overall lower. This could make our\nstudy results better or worse depending on context.\n* Called **right censored** data because the RIGHT endpoint of the interval is\nunknown.\n\n### Lab assays are also commonly censored.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/left censoring example-1.png){width=960}\n:::\n:::\n\n\n\n* Immunological measurements often have a lower limit to the titer that can\nbe accurately measured. (The *lower limit of detection, LoD*.)\n* In this plot, if the true (\"latent\") antibody titer is less than 2 is\nrecorded as 2. Although those values are really in $(0, 2]$ on the natural\nscale, or $(-\\infty, 1]$ on the log scale.\n* So the LoD measurements are **left censored** because the left endpoint of the\ninterval is unknown.\n\n### Data can be simultaneously left and right censored.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/left and right censoring example-1.png){width=960}\n:::\n:::\n\n\n\n* The same assay can have both a lower limit and an upper limit of detection,\nso a variable can contain left censored and right censored values.\n* However, a specific observation can only be left censored or right censored.\n\n### Interval censoring provides a more general framework.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/interval censoring example-1.png){width=960}\n:::\n:::\n\n\n\n* Under **interval censoring**, there are a discrete number of possible values we\ncan observe, even though the underlying quantity of interest is continuous.\n* Influenza HAI titer and other serial dilution assays are often interval censored.\n* Many interval-censored assays also have limits of detection, like the one\nshown here.\n\n### For example, HAI titers have a lower LoD and are interval censored.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/HAI example-1.png){width=960}\n:::\n:::\n\n\n\n## Censored data biases models.\n\n* Consider the simple example of a standard normal random variable, $z \\overset{\\mathrm{iid}}{\\sim} \\text{Normal}(0, 1)$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(8980)\nsimple_ex <- tibble::tibble(\n\tz = rnorm(1000)\n)\n```\n:::\n\n\n\n* We know that if we estimate\nthe mean and variance using the normal methods, we'll get an asymptotically\ngood estimate as $n \\to \\infty$.\n* What happens if we impose a lower limit of detection at, say, $z = -1$? We\ncan write this as\n\n$$\nY_i = \\begin{cases}\n-1, & Z_i < -1 \\\\\nZ_i, & Z_i \\geq -1\n\\end{cases}.\n$$\n\n* And then simulate the censoring.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_ex <- simple_ex |>\n\tdplyr::mutate(\n\t\tlod = rep(-1, times = 1000),\n\t\ty = ifelse(z < lod, lod, z)\n\t)\n```\n:::\n\n\n\n* The distributions are clearly different.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(simple_ex) +\n\tgeom_histogram(\n\t\taes(x = z, y = after_stat(density), fill = \"latent\"),\n\t\tboundary = 0,\n\t\tcolor = \"black\",\n\t\tbinwidth = 0.1\n\t) +\n\tgeom_histogram(\n\t\taes(x = y, y = after_stat(density), fill = \"observed\"),\n\t\tboundary = 0,\n\t\tcolor = \"black\",\n\t\tbinwidth = 0.1\n\t) +\n\tscale_fill_manual(\n\t\tvalues = c(\"#E69F0050\", \"#56B4E950\"),\n\t\tname = NULL\n\t)\n```\n\n::: {.cell-output-display}\n![](main_files/figure-revealjs/simple example plot-1.png){width=960}\n:::\n:::\n\n\n\n* The standard MLE (assuming a normal distribution) doesn't work.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_ex |>\n\tdplyr::select(-lod) |>\n\ttidyr::pivot_longer(c(z, y)) |>\n\tdplyr::summarise(\n\t\tmean = mean(value),\n\t\tvariance = var(value),\n\t\t.by = c(name)\n\t) |>\n\tdplyr::mutate(\n\t\tvariable = ifelse(name == \"z\", \"latent\", \"censored\"),\n\t\t.keep = \"unused\",\n\t\t.before = dplyr::everything()\n\t) |>\n\ttibble::add_row(\n\t\tvariable = \"truth\",\n\t\tmean = 0,\n\t\tvariance = 1,\n\t\t.before = 1\n\t) |>\n\tknitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|variable |  mean| variance|\n|:--------|-----:|--------:|\n|truth    | 0.000|    1.000|\n|latent   | 0.046|    0.929|\n|censored | 0.117|    0.717|\n\n\n:::\n:::\n\n\n\n* We can see that the mean is overestimated and the variance is underestimated. \n* If we did standard $t$-tests for the mean, and $\\chi^2$-test for the variance,\nwe would get that the latent variable gives the correct decision (fail to\nreject), while the censored variable would reject both.\n* As the proportion of data that are censored increases, the bias gets worse (proof not shown, but true in general).\n\n::: {style=\"font-size: 50%;\"}\n\n* Mean estimate: $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^nx_i$.\n* Var estimate: $s^2_x = \\frac{1}{n-1}\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2$.\n\n:::\n\n## How do we fix it?\n\n* We need to modify the likelihood. Recall that for a parametric model assuming\nmutually independent observations, the likelihood of the sample is\n\n$$\n\\mathcal{L}(\\vec{\\theta} \\mid \\vec{x}) = \\prod_{i=1}^n f_X\\left(x_i \\mid \\vec{\\theta}\\right)\n$$\n\nwhere $f_X$ is the probability density function of random variable $X$ imposed\nby some parametric model.\n* This is easy to do for our latent variable,\n\n$$z \\overset{\\mathrm{iid}}{\\sim} \\text{Normal}(0, 1).$$\n* Since we know what the normal PDF looks like, we can write\n\n$$\n\\mathcal{L}\\left( \\begin{bmatrix}\\mu \\\\ \\sigma\\end{bmatrix} \\mid \\vec{z}\\right) = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right),\n$$\nand we simultaneously minimize over both $\\mu$ and $\\sigma$.\n* Two paths from here: add priors and sample from the posterior distribution of $\\vec{\\theta}$, or find the argmax w.r.t. $\\vec{\\theta}$.\n\n### It's a bit harder for censored data.\n\n* For our censored problem, we first need to write out the pdf.\n* Let $C_i$ be an indicator for whether $Z_i$ is censored, that is,\n\n$$\nC_i = \\begin{cases}\n1, & Z_i < -1 \\\\\n0, & Z_i \\geq -1\n\\end{cases}.\n$$\n\n* Joke about the Lesbegue decomposition theorem\n* The central observation we need to make is that (skipping over technical issues with the point probability of a continuous R.V.)\n\n$$\n\\mathrm{Pr}\\left(Y_i =-1\\right) = \\int_{-\\infty}^{-1}f_{Z}\\left(y_i \\mid \\vec{\\theta}\\right)\\ dy_i = F_Z\\left(-1 \\mid \\vec{\\theta}\\right).\n$$\n\n* Then we can write\n\n$$\n\\mathcal{L}\\left(y_i \\mid \\vec{\\theta}\\right) = f_Z\\left(y_i \\mid \\vec{\\theta}\\right)^{c_i - 1} \\cdot F_Z\\left(-1 \\mid \\vec{\\theta}\\right)^{c_i}.\n$$\n\n* Now, we could maximize the censored data likelihood ourselves.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nex_ll <- function(mu, sigma, x, L) {\n\tvals <- ifelse(\n\t\tx > -1,\n\t\tdnorm(x, mu, sigma, log = TRUE),\n\t\tpnorm(L, mu, sigma, log.p = TRUE)\n\t)\n\t\n\tout <- sum(vals)\n\treturn(out)\n}\n\nsimple_ll <- tidyr::expand_grid(\n\tmu = seq(-2, 2, 0.01),\n\tsigma = seq(0, 2, 0.01)\n) |>\n\tdplyr::mutate(\n\t\tll = purrr::map2_dbl(\n\t\t\tmu, sigma,\n\t\t\t\\(m, s) ex_ll(m, s, x = simple_ex$y, L = -1)\n\t\t)\n\t)\n\nmle <- simple_ll |> dplyr::slice_max(ll)\nmle |> knitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|   mu| sigma|        ll|\n|----:|-----:|---------:|\n| 0.05|  0.96| -1340.695|\n\n\n:::\n:::\n\n\n\n* Or we can use something that comes with R (much more convenient cause we\ncan get CIs, etc., but can be tricky).\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurv_fit <- survival::survreg(\n\tsurvival::Surv(y, y > lod, type = \"left\") ~ 1,\n\tdata = simple_ex,\n\tdist = \"gaussian\"\n)\n\nsurv_est <-\n\tbroom::tidy(surv_fit) |>\n\tdplyr::mutate(\n\t\tterm = ifelse(term == \"(Intercept)\", \"mean\", \"variance\"),\n\t\tlwr = estimate - 1.96 * std.error,\n\t\tupr = estimate + 1.96 * std.error,\n\t) |>\n\tdplyr::select(term, estimate, lwr, upr) |>\n\tas.data.frame()\n\nsurv_est[2, 2:4] <- exp(surv_est[2, 2:4])\nknitr::kable(surv_est, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term     | estimate|    lwr|   upr|\n|:--------|--------:|------:|-----:|\n|mean     |    0.049| -0.012| 0.109|\n|variance |    0.960|  0.914| 1.008|\n\n\n:::\n:::\n\n\n\n* We can also easily fit the model in a Bayesian framework using `brms`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms_dat <- simple_ex\nbrms_dat$c <- ifelse(brms_dat$z < brms_dat$lod, \"left\", \"none\")\n\nbrms_fit <-\n\tbrms::brm(\n\t\tformula = y | cens(c) ~ 1,\n\t\tdata = brms_dat,\n\t\tfamily = gaussian()\n\t)\n\nsummary(brms_fit)\n```\n:::\n\n\n\n## Example 1: HAI\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "main_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}