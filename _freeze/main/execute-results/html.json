{
  "hash": "655cbcedebf1c43b105bcf527232213a",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Linear models and censored data\"\nauthor: \"Zane Billings\"\ndate: last-modified\ndate-format: iso\nformat:\n  revealjs:\n    scrollable: true\n    auto-stretch: false\n    smaller: true\n    slide-number: true\nexecute: \n  freeze: auto\n  echo: true\nfilters:\n  - include-code-files\n---\n\n\n\n\n\n\n## Censored data are partially known.\n\n* In general if a data point $x$ is censored, we don't know the exact\nvalue of $x$, but we know that $x \\in \\left(x_L, x_U\\right)$.\n* The most common example in epidemiology is a time-to-event outcome in\nsurvival analysis.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/right censored example figure-1.png){width=960}\n:::\n:::\n\n\n\n\n* Participants 2, 6, and 10 have event times in $\\left[\\text{May 16}, \\infty\\right).$\n* If we just throw those people out, or count their event date as May 16, we\nbias the distribution of event times to be overall lower. This could make our\nstudy results better or worse depending on context.\n* Called **right censored** data because the RIGHT endpoint of the interval is\nunknown.\n\n### Lab assays are also commonly censored.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/left censoring example-1.png){width=960}\n:::\n:::\n\n\n\n* Immunological measurements often have a lower limit to the titer that can\nbe accurately measured. (The lower limit of detection, LoD.)\n* In this plot, if the true (\"latent\") antibody titer is less than 2 is\nrecorded as 2. Although those values are really in $(0, 2]$ on the natural\nscale, or $(-\\infty, 1]$ on the log scale.\n* So the LoD measurements are **left censored** because the left endpoint of the\ninterval is unknown.\n\n### Data can be simultaneously left and right censored.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/left and right censoring example-1.png){width=960}\n:::\n:::\n\n\n\n* The same assay can have both a lower limit and an upper limit of detection,\nso a variable can contain left censored and right censored values.\n* However, a specific observation can only be left censored or right censored.\n\n### Interval censoring provides a more general framework.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/interval censoring example-1.png){width=960}\n:::\n:::\n\n\n\n* Under **interval censoring**, there are a discrete number of possible values we\ncan observe, even though the underlying quantity of interest is continuous.\n* Influenza HAI titer and other serial dilution assays are often interval censored.\n* Many interval-censored assays also have limits of detection, like the one\nshown here.\n\n### For example, HAI titers have a lower LoD and are interval censored.\n\n\n\n::: {.cell}\n::: {.cell-output-display}\n![](main_files/figure-revealjs/HAI example-1.png){width=960}\n:::\n:::\n\n\n\n## Censored data biases models.\n\n* Consider the simple example of a standard normal random variable, $z \\overset{\\mathrm{iid}}{\\sim} \\text{Normal}(0, 1)$.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(8980)\nsimple_ex <- tibble::tibble(\n\tz = rnorm(1000)\n)\n```\n:::\n\n\n\n* We know that if we estimate\nthe mean and variance using the normal methods, we'll get an asymptotically\ngood estimate as $n \\to \\infty$.\n* What happens if we impose a lower limit of detection at, say, $z = -1$? We\ncan write this as\n\n$$\nY_i = \\begin{cases}\n-1, & Z_i < -1 \\\\\nZ_i, & Z_i \\geq -1\n\\end{cases}.\n$$\n\n* And then simulate the censoring.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_ex <- simple_ex |>\n\tdplyr::mutate(\n\t\tlod = rep(-1, times = 1000),\n\t\ty = ifelse(z < lod, lod, z)\n\t)\n```\n:::\n\n\n\n* The distributions are clearly different.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(simple_ex) +\n\tgeom_histogram(\n\t\taes(x = z, y = after_stat(density), fill = \"latent\"),\n\t\tboundary = 0,\n\t\tcolor = \"black\",\n\t\tbinwidth = 0.1\n\t) +\n\tgeom_histogram(\n\t\taes(x = y, y = after_stat(density), fill = \"observed\"),\n\t\tboundary = 0,\n\t\tcolor = \"black\",\n\t\tbinwidth = 0.1\n\t) +\n\tscale_fill_manual(\n\t\tvalues = c(\"#E69F0050\", \"#56B4E950\"),\n\t\tname = NULL\n\t)\n```\n\n::: {.cell-output-display}\n![](main_files/figure-revealjs/simple example plot-1.png){width=960}\n:::\n:::\n\n\n\n* The standard MLE (assuming a normal distribution) doesn't work.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsimple_ex |>\n\tdplyr::select(-lod) |>\n\ttidyr::pivot_longer(c(z, y)) |>\n\tdplyr::summarise(\n\t\tmean = mean(value),\n\t\tvariance = var(value),\n\t\t.by = c(name)\n\t) |>\n\tdplyr::mutate(\n\t\tvariable = ifelse(name == \"z\", \"latent\", \"censored\"),\n\t\t.keep = \"unused\",\n\t\t.before = dplyr::everything()\n\t) |>\n\ttibble::add_row(\n\t\tvariable = \"truth\",\n\t\tmean = 0,\n\t\tvariance = 1,\n\t\t.before = 1\n\t) |>\n\tknitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|variable |  mean| variance|\n|:--------|-----:|--------:|\n|truth    | 0.000|    1.000|\n|latent   | 0.046|    0.929|\n|censored | 0.117|    0.717|\n\n\n:::\n:::\n\n\n\n* We can see that the mean is overestimated and the variance is underestimated. \n* If we did standard $t$-tests for the mean, and $\\chi^2$-test for the variance,\nwe would get that the latent variable gives the correct decision (fail to\nreject), while the censored variable would reject both.\n* As the proportion of data that are censored increases, the bias gets worse (proof not shown, but true in general).\n\n::: {style=\"font-size: 50%;\"}\n\n* Mean estimate: $\\bar{x} = \\frac{1}{n} \\sum_{i=1}^nx_i$.\n* Var estimate: $s^2_x = \\frac{1}{n-1}\\sum_{i=1}^n\\left(x_i-\\bar{x}\\right)^2$.\n\n:::\n\n## How do we fix it?\n\n* We need to modify the likelihood. Recall that for a parametric model assuming\nmutually independent observations, the likelihood of the sample is\n\n$$\n\\mathcal{L}(\\vec{\\theta} \\mid \\vec{x}) = \\prod_{i=1}^n f_X\\left(x_i \\mid \\vec{\\theta}\\right)\n$$\n\nwhere $f_X$ is the probability density function of random variable $X$ imposed\nby some parametric model.\n\n* This is easy to do for our latent variable. Since we know what the normal PDF looks like, we can write\n\n$$\n\\mathcal{L}\\left( \\begin{bmatrix}\\mu \\\\ \\sigma\\end{bmatrix} \\mid \\vec{z}\\right) = \\prod_{i=1}^n \\frac{1}{\\sigma\\sqrt{2\\pi}}\\exp\\left(-\\frac{1}{2}\\left(\\frac{x-\\mu}{\\sigma}\\right)^2\\right),\n$$\nand we simultaneously maximize (or typically minimize the negative log-likelihood) over both $\\mu$ and $\\sigma$.\n\n* Two paths from here: add priors and sample from the posterior distribution of $\\vec{\\theta}$, or find the argmax w.r.t. $\\vec{\\theta}$.\n\n### It's a bit harder for censored data.\n\n* For our censored problem, we first need to write out the pdf.\n* Let $C_i$ be an indicator for whether $Z_i$ is censored, that is,\n\n$$\nC_i = \\begin{cases}\n1, & Z_i < -1 \\\\\n0, & Z_i \\geq -1\n\\end{cases}.\n$$\n\n* The central observation we need to make is that (skipping over technical issues with the point probability of a continuous R.V.)\n\n$$\n\\mathrm{Pr}\\left(Y_i =-1\\right) = \\mathrm{Pr}\\left(Z_i \\leq -1\\right)=\\int_{-\\infty}^{-1}f_{Z}\\left(y_i \\mid \\vec{\\theta}\\right)\\ dy_i = F_Z\\left(-1 \\mid \\vec{\\theta}\\right).\n$$\n\n* This induces a point mass in the probability distribution of $Y$.\n* Joke about the Lesbegue decomposition theorem\n* Using the indicator $C$, we can write the likelihood of $Y_i$ as\n\n$$\n\\mathcal{L}\\left(y_i \\mid \\vec{\\theta}\\right) = f_Z\\left(y_i \\mid \\vec{\\theta}\\right)^{c_i - 1} \\cdot F_Z\\left(-1 \\mid \\vec{\\theta}\\right)^{c_i}.\n$$\n\n* Now, we could maximize the censored data likelihood ourselves.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nex_ll <- function(mu, sigma, x, L) {\n\tvals <- ifelse(\n\t\tx > L,\n\t\tdnorm(x, mu, sigma, log = TRUE),\n\t\tpnorm(L, mu, sigma, log.p = TRUE)\n\t)\n\t\n\tout <- sum(vals)\n\treturn(out)\n}\n\nsimple_ll <-\n\ttidyr::expand_grid(\n\t\tmu = seq(-2, 2, 0.01),\n\t\tsigma = seq(0, 2, 0.01)\n\t) |>\n\tdplyr::mutate(\n\t\tll = purrr::map2_dbl(\n\t\t\tmu, sigma,\n\t\t\t\\(m, s) ex_ll(m, s, x = simple_ex$y, L = -1)\n\t\t)\n\t)\n\nsimple_ll |>\n\tdplyr::slice_max(ll) |>\n\tknitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|   mu| sigma|        ll|\n|----:|-----:|---------:|\n| 0.05|  0.96| -1340.695|\n\n\n:::\n:::\n\n\n\n* There are also built in methods in base R (specifically the `survival`)\npackage which implement this likelihood adjustment for us.\n* This is more convenient cause we can easily get CIs and other statistical\nstuff, but we have to use the insane syntax of `survival`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsurv_fit <- survival::survreg(\n\tsurvival::Surv(y, y > lod, type = \"left\") ~ 1,\n\tdata = simple_ex,\n\tdist = \"gaussian\"\n)\n\nsurv_est <-\n\tbroom::tidy(surv_fit) |>\n\tdplyr::mutate(\n\t\tterm = ifelse(term == \"(Intercept)\", \"mean\", \"variance\"),\n\t\tlwr = estimate - 1.96 * std.error,\n\t\tupr = estimate + 1.96 * std.error,\n\t) |>\n\tdplyr::select(term, estimate, lwr, upr) |>\n\tas.data.frame()\n\nsurv_est[2, 2:4] <- exp(surv_est[2, 2:4])\nknitr::kable(surv_est, digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|term     | estimate|    lwr|   upr|\n|:--------|--------:|------:|-----:|\n|mean     |    0.049| -0.012| 0.109|\n|variance |    0.960|  0.914| 1.008|\n\n\n:::\n:::\n\n\n\n* We can also easily fit the model in a Bayesian framework using `brms`.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nbrms_dat <- simple_ex\nbrms_dat$c <- ifelse(brms_dat$z < brms_dat$lod, \"left\", \"none\")\n\nbrms_fit <-\n\tbrms::brm(\n\t\tformula = y | cens(c) ~ 1,\n\t\tdata = brms_dat,\n\t\tfamily = gaussian()\n\t)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.000226 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 2.26 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 0.522 seconds (Warm-up)\nChain 1:                0.573 seconds (Sampling)\nChain 1:                1.095 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.000451 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 4.51 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 0.542 seconds (Warm-up)\nChain 2:                0.511 seconds (Sampling)\nChain 2:                1.053 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.000139 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 1.39 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 0.533 seconds (Warm-up)\nChain 3:                0.56 seconds (Sampling)\nChain 3:                1.093 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.000122 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 1.22 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 0.532 seconds (Warm-up)\nChain 4:                0.591 seconds (Sampling)\nChain 4:                1.123 seconds (Total)\nChain 4: \n```\n\n\n:::\n\n```{.r .cell-code}\nsummary(brms_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: y | cens(c) ~ 1 \n   Data: brms_dat (Number of observations: 1000) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     0.05      0.03    -0.01     0.11 1.00     2811     2246\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     0.96      0.02     0.92     1.01 1.00     3375     2695\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n## Example 1: HAI\n\n* In this example, I'll discuss one of my current research interests and work\nthrough an example with predictors.\n* We'll also discuss a model with a ratio of censored variables as the outcome.\n\n### UGAFluVac data description\n\n* Data from Ted Ross, collected at three study sites (PA, FL, and UGA) between\nSeptember 2014 and April 2017. All individuals received a vaccine containing\nA/H1N1/California/NUMBER/2009-like virus.\n* Each individual gave a study sample before vaccination and 21 days after.\nLab workers did HAI assays against a large panel of historical strains.\n* We calculated the antigenic distance between each historical\nstrain and the vaccine strain, so we have an \"antibody landscape\" for \neach individual in the study.\n* For the purpose of this analysis, we ignore repeated measurements of the\nsame individual in subsequent years and pretend they are new people.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(101)\nsampled_ids <- sample(hai_dat$uniq_id |> unique(), size = 10)\n\nhai_dat |>\n\tdplyr::filter(uniq_id %in% sampled_ids) |>\n\tggplot() +\n\taes(x = norm_dist, y = 2 ^ posttiter, group = id, color = factor(id)) +\n\tgeom_line(\n\t\tshow.legend = FALSE\n\t) +\n\tscale_color_viridis_d(begin = 0.1, end = 0.9) +\n\tscale_y_continuous(\n\t\ttrans = \"log2\",\n\t\tbreaks = 2 ^ seq(0, 10),\n\t\tlabels = 5 * 2 ^ seq(0, 10)\n\t) +\n\tlabs(\n\t\ty = \"Post-vaccination HAI titer\",\n\t\tx = \"Normalized antigenic distance\",\n\t\ttitle = \"Antibody landscape for 10 randomly sampled individuals\"\n\t)\n```\n\n::: {.cell-output-display}\n![](main_files/figure-revealjs/hai plot-1.png){width=960}\n:::\n:::\n\n\n\n* Instead of looking at this as a trajectory, we can also just make a scatter\nplot since random and group effects are beyond today's discussion.\n\n* HAI titer is a serial dilution assay with a limit of detection at 5, so it\nhas both interval censoring and left censoring (which defines a specific\ninterval into which the outcome can be censored).\n* Even with just a basic estimate of the mean post-titer value, adjusting\nfor the different types of censoring makes a difference.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhai_mean_naive <-\n\thai_dat |>\n\tdplyr::summarise(mean_cl_boot(posttiter)) |>\n\tdplyr::mutate(across(everything(), hai_inverse)) |>\n\tdplyr::mutate(method = \"Naive\", .before = everything())\n\nsurv_fit_hai_simple <-\n\tsurvival::survreg(\n\t\tsurvival::Surv(posttiter, posttiter > 0, type = 'left') ~ 1,\n\t\tdata = hai_dat,\n\t\tdist = \"gaussian\"\n\t)\n\nhai_mean_lod <-\n\tbroom::tidy(surv_fit_hai_simple, conf.int = TRUE) |>\n\tdplyr::filter(term == \"(Intercept)\") |>\n\tdplyr::mutate(\n\t\ty = hai_inverse(estimate),\n\t\tymin = hai_inverse(conf.low),\n\t\tymax = hai_inverse(conf.high),\n\t\t.keep = \"none\"\n\t) |>\n\tdplyr::mutate(method = \"LoD only\", .before = everything())\n\nhai_dat_interval <-\n\thai_dat |>\n\tdplyr::mutate(\n\t\tpost_min = ifelse(posttiter == 0, -Inf, posttiter),\n\t\tpost_max = ifelse(posttiter == 0, 0, posttiter + 1)\n\t)\n\nhai_dat_interval |>\n\tdplyr::select(posttiter, post_min, post_max) |>\n\thead() |>\n\tknitr::kable(digits = 3, caption = \"interval2 format example\")\n```\n\n::: {.cell-output-display}\n\n\nTable: interval2 format example\n\n| posttiter| post_min| post_max|\n|---------:|--------:|--------:|\n|         2|        2|        3|\n|         0|     -Inf|        0|\n|         0|     -Inf|        0|\n|         1|        1|        2|\n|         2|        2|        3|\n|         0|     -Inf|        0|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsurv_fit_hai_interval2 <-\n\tsurvival::survreg(\n\t\tsurvival::Surv(post_min, post_max, type = 'interval2') ~ 1,\n\t\tdata = hai_dat_interval,\n\t\tdist = \"gaussian\"\n\t)\n\nhai_mean_int <-\n\tbroom::tidy(surv_fit_hai_interval2, conf.int = TRUE) |>\n\tdplyr::filter(term == \"(Intercept)\") |>\n\tdplyr::mutate(\n\t\ty = hai_inverse(estimate),\n\t\tymin = hai_inverse(conf.low),\n\t\tymax = hai_inverse(conf.high),\n\t\t.keep = \"none\"\n\t) |>\n\tdplyr::mutate(method = \"Interval censoring\", .before = everything())\n\ndplyr::bind_rows(hai_mean_naive, hai_mean_lod, hai_mean_int) |>\n\tdplyr::rename(\n\t\testimate = y,\n\t\tlower = ymin,\n\t\tupper = ymax\n\t) |>\n\tknitr::kable(digits = 3)\n```\n\n::: {.cell-output-display}\n\n\n|method             | estimate|  lower|  upper|\n|:------------------|--------:|------:|------:|\n|Naive              |   21.221| 20.724| 21.744|\n|LoD only           |   15.504| 14.995| 16.031|\n|Interval censoring |   19.190| 18.499| 19.907|\n\n\n:::\n:::\n\n\n\n* Now let's compare models for our actual research question, which is the\neffect of antigenic distance on the post-vaccination titer. That is, we want\nto summarize all of the individual antibody landscapes by fitting a line\nthrough them.\n\n* First, we'll look at the naive fit, which doesn't adjust for censoring.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhai_simple_fit <-\n\tlm(\n\t\tposttiter ~ norm_dist,\n\t\tdata = hai_dat_interval\n\t)\n\nbroom::tidy(hai_simple_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)     4.12    0.0474      86.9       0\n2 norm_dist      -2.89    0.0634     -45.5       0\n```\n\n\n:::\n:::\n\n\n\n* Next we'll look at a frequentist fit which adjusts for censoring using\nthe survival package.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhai_surv_fit <-\n\t\tsurvival::survreg(\n\t\tsurvival::Surv(post_min, post_max, type = 'interval2') ~ norm_dist,\n\t\tdata = hai_dat_interval,\n\t\tdist = \"gaussian\"\n\t)\n\nbroom::tidy(hai_surv_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 3 × 5\n  term        estimate std.error statistic p.value\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)    4.73    0.0707       67.0       0\n2 norm_dist     -3.94    0.0958      -41.1       0\n3 Log(scale)     0.936   0.00842     111.        0\n```\n\n\n:::\n:::\n\n\n\n* Wow! When we do the interval censoring adjustment, the effect of antigenic\ndistance is stronger than we thought under the naive model!\n\n* Next we'll look at a Bayesian fit using `brms`. We have to do a bit more\ncleaning though because of the format that `brms` uses, which isn't the same\nas `interval2` form.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhai_dat_interval$cens <-\n\tifelse(hai_dat_interval$post_min == -Inf, \"left\", \"interval\")\n\nhai_dat_interval |>\n\tdplyr::select(posttiter, cens, post_max) |>\n\thead() |>\n\tknitr::kable(digits = 3, caption = \"brms censoring format\")\n```\n\n::: {.cell-output-display}\n\n\nTable: brms censoring format\n\n| posttiter|cens     | post_max|\n|---------:|:--------|--------:|\n|         2|interval |        3|\n|         0|left     |        0|\n|         0|left     |        0|\n|         1|interval |        2|\n|         2|interval |        3|\n|         0|left     |        0|\n\n\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nhai_brms_fit <-\n\tbrms::brm(\n\t\tposttiter | cens(cens, post_max) ~ norm_dist,\n\t\tfamily = gaussian(),\n\t\tdata = hai_dat_interval\n\t)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).\nChain 1: \nChain 1: Gradient evaluation took 0.007092 seconds\nChain 1: 1000 transitions using 10 leapfrog steps per transition would take 70.92 seconds.\nChain 1: Adjust your expectations accordingly!\nChain 1: \nChain 1: \nChain 1: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 1: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 1: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 1: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 1: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 1: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 1: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 1: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 1: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 1: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 1: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 1: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 1: \nChain 1:  Elapsed Time: 28.077 seconds (Warm-up)\nChain 1:                26.848 seconds (Sampling)\nChain 1:                54.925 seconds (Total)\nChain 1: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).\nChain 2: \nChain 2: Gradient evaluation took 0.005919 seconds\nChain 2: 1000 transitions using 10 leapfrog steps per transition would take 59.19 seconds.\nChain 2: Adjust your expectations accordingly!\nChain 2: \nChain 2: \nChain 2: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 2: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 2: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 2: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 2: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 2: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 2: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 2: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 2: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 2: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 2: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 2: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 2: \nChain 2:  Elapsed Time: 28.435 seconds (Warm-up)\nChain 2:                27.909 seconds (Sampling)\nChain 2:                56.344 seconds (Total)\nChain 2: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).\nChain 3: \nChain 3: Gradient evaluation took 0.005914 seconds\nChain 3: 1000 transitions using 10 leapfrog steps per transition would take 59.14 seconds.\nChain 3: Adjust your expectations accordingly!\nChain 3: \nChain 3: \nChain 3: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 3: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 3: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 3: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 3: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 3: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 3: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 3: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 3: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 3: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 3: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 3: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 3: \nChain 3:  Elapsed Time: 30.109 seconds (Warm-up)\nChain 3:                25.397 seconds (Sampling)\nChain 3:                55.506 seconds (Total)\nChain 3: \n\nSAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).\nChain 4: \nChain 4: Gradient evaluation took 0.006433 seconds\nChain 4: 1000 transitions using 10 leapfrog steps per transition would take 64.33 seconds.\nChain 4: Adjust your expectations accordingly!\nChain 4: \nChain 4: \nChain 4: Iteration:    1 / 2000 [  0%]  (Warmup)\nChain 4: Iteration:  200 / 2000 [ 10%]  (Warmup)\nChain 4: Iteration:  400 / 2000 [ 20%]  (Warmup)\nChain 4: Iteration:  600 / 2000 [ 30%]  (Warmup)\nChain 4: Iteration:  800 / 2000 [ 40%]  (Warmup)\nChain 4: Iteration: 1000 / 2000 [ 50%]  (Warmup)\nChain 4: Iteration: 1001 / 2000 [ 50%]  (Sampling)\nChain 4: Iteration: 1200 / 2000 [ 60%]  (Sampling)\nChain 4: Iteration: 1400 / 2000 [ 70%]  (Sampling)\nChain 4: Iteration: 1600 / 2000 [ 80%]  (Sampling)\nChain 4: Iteration: 1800 / 2000 [ 90%]  (Sampling)\nChain 4: Iteration: 2000 / 2000 [100%]  (Sampling)\nChain 4: \nChain 4:  Elapsed Time: 27.687 seconds (Warm-up)\nChain 4:                29.666 seconds (Sampling)\nChain 4:                57.353 seconds (Total)\nChain 4: \n```\n\n\n:::\n:::\n\n\n\nAfter sampling we can take a look at the model fit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(hai_brms_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n Family: gaussian \n  Links: mu = identity; sigma = identity \nFormula: posttiter | cens(cens, post_max) ~ norm_dist \n   Data: hai_dat_interval (Number of observations: 11923) \n  Draws: 4 chains, each with iter = 2000; warmup = 1000; thin = 1;\n         total post-warmup draws = 4000\n\nRegression Coefficients:\n          Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nIntercept     4.74      0.07     4.60     4.87 1.00     3269     3011\nnorm_dist    -3.94      0.10    -4.13    -3.75 1.00     3078     2808\n\nFurther Distributional Parameters:\n      Estimate Est.Error l-95% CI u-95% CI Rhat Bulk_ESS Tail_ESS\nsigma     2.55      0.02     2.51     2.59 1.00     3574     2651\n\nDraws were sampled using sampling(NUTS). For each parameter, Bulk_ESS\nand Tail_ESS are effective sample size measures, and Rhat is the potential\nscale reduction factor on split chains (at convergence, Rhat = 1).\n```\n\n\n:::\n:::\n\n\n\n* Finally we can do a fit using our own Stan code. This is a bit slower than\nthe `brms` version because `brms` has a lot of Stan code improvements that\nspeed up the code but are more difficult to read and write for me. However,\nif we include censored predictors we'll be forced to write our own Stan code.\n\n```{.stan include=\"stan/hai-censored-outcome.stan\"}\n```\n\nFirst we have to compile the Stan code.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmod_pth <- here::here(\"stan\", \"hai-censored-outcome.stan\")\nhai_stan_mod <- cmdstanr::cmdstan_model(mod_pth, compile = FALSE)\nhai_stan_mod$compile(pedantic = TRUE, force_recompile = TRUE)\n```\n:::\n\n\n\nWe also have to format our data in a specific way for Stan. Stan only accepts\nnumeric variables, and the input needs to be a list because the inputs can\nvary in length.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhai_stan_dat <-\n\thai_dat_interval |>\n\tdplyr::select(\n\t\tx = norm_dist,\n\t\ty_l = post_min,\n\t\ty_u = post_max,\n\t\tc = cens\n\t) |>\n\t# Convert cens to integer following brms convention\n\tdplyr::mutate(\n\t\tc = ifelse(c == \"left\", -1L, 2L)\n\t) |>\n\tas.list()\nhai_stan_dat$N <- nrow(hai_dat_interval)\n```\n:::\n\n\n\nNow we can sample from the model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhai_stan_fit <- hai_stan_mod$sample(\n\thai_stan_dat,\n\tseed = 923154,\n\tparallel_chains = 4,\n\titer_warmup = 1000,\n\titer_sampling = 1000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 finished in 89.4 seconds.\nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 92.4 seconds.\nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 95.4 seconds.\nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 98.3 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 93.9 seconds.\nTotal execution time: 98.6 seconds.\n```\n\n\n:::\n:::\n\n\n\nAnd finally we can check the model fit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhai_stan_fit\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n variable      mean    median   sd  mad        q5       q95 rhat ess_bulk\n    lp__  -23099.66 -23099.30 1.23 1.04 -23102.00 -23098.30 1.00     1445\n    a          4.72      4.73 0.07 0.07      4.60      4.84 1.00     1362\n    b         -3.93     -3.93 0.10 0.10     -4.09     -3.76 1.00     1374\n    sigma      2.55      2.55 0.02 0.02      2.51      2.58 1.00     1844\n ess_tail\n     2089\n     1427\n     1287\n     1828\n```\n\n\n:::\n:::\n\n\n\n## Dealing with censored predictors\n\n* Now, we might also want to deal with a censored predictor. Unfortunately, this\nis more difficult than dealing with a censored outcome.\n* In a standard linear regression model, **we assume the predictors are\nconstant**. So if we want to incorporate censoring, we have to assume they\ncan be observed with some type of error.\n* The easiest way to do this is to assume a parametric model for the predictor,\ne.g.\n\n$$\n\\begin{aligned}\ny_i &\\sim \\mathrm{Normal}\\left(\\alpha + \\beta \\cdot x_i, \\sigma^2\\right) \\\\\nx_i &\\sim \\mathrm{Normal}\\left(\\mu_x, \\sigma^2_x\\right)\n\\end{aligned}\n$$\n\n* **This specific model is not identifiable without further constraints!**\n* If we assume that the $y_i$ are conditionally independent of $x_i$ (this is the typical linear regression assumption), then\n\n$$\n\\mathcal{L}\\left(\\theta \\mid x, y\\right) = \\prod_{i=1}^n f_X(x \\mid \\theta)\\cdot f_{Y \\mid X}\\left(y \\mid \\theta, X = x\\right).\n$$\n\n* The main issue here is that's not necessarily true, because\n\n$$\nf_{X, Y}(x, y) \\neq f_X(x)\\cdot f_Y(y)\n$$\nin general.\n\n* Usually people are OK\nwith assuming that $x$ is a constant, even when that doesn't make sense, so\nI'm pretty much fine with assuming marginal independence.\n* However, there's also the option of using a\n[copula function](https://en.wikipedia.org/wiki/Copula_(probability_theory)) to construct the joint likelihood, which is not too hard if you\nuse a Bayesian model. But choosing a copula is a whole additional issue.\n* **This likelihood can be hard to optimize.** AFAIK, there are not any\npackages for `R` that deal with censored predictors in a frequentist\nperspective, although you could numerically optimize it. You could also get CIs\nusing bootstrapping or maybe a profile method.\n* However, joint modeling of $x$ and $y$ is not too difficult to do in Stan,\nso that's the approach we'll take.\n\n## Example 2: norovirus\n\n* This is one of Savannah's current projects, so I don't know too much about the\ndata.\n* But it's from a norovirus vaccine clinical trial, and I'll be looking at\nthe effect of an antibody on the outcome, including an interaction with the\neffect of the treatment.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnov_dat <- readr::read_rds(here::here(\"data\", \"nov-clean.Rds\"))\n\ndplyr::glimpse(nov_dat)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 107\nColumns: 3\n$ treatment <dbl> 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, …\n$ protected <dbl> 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, …\n$ gii_pgm   <dbl> 1569.318, 566.593, 946.028, 570.214, 1335.983, 505.337, 291.…\n```\n\n\n:::\n:::\n\n\n\n* All I really know about the data is that a bunch of people got the candidate\nvaccine, then got norovirus challenged, and they were followed up to see if\nthey got infected. Savannah knows all the details about the antibody assay\nI'm looking at and the timing and stuff. Those details aren't really necessary\nfor what I'm trying to do here.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(nov_dat) +\n\taes(x = log(gii_pgm), y = protected, color = factor(treatment)) +\n\tgeom_point(position = position_jitter(height = 0.15, width = 0))\n```\n\n::: {.cell-output-display}\n![](main_files/figure-revealjs/nov dat plot-1.png){width=960}\n:::\n:::\n\n\n\n* Since there is a big clump of values at $92$, we'll use that as the limit of\ndetection for the assay, although I think Savannah had some other issues with\nthe LoD. However, for our purposes this will just be a left-censored predictor\nwith the LoD at $92$.\n* Explain the model:\n\n$$\n\\begin{align*}\ny_i &\\sim \\text{Bernoulli}\\left(p_i\\right) \\\\\n\\mathrm{logit}(p_i) &= \\beta_{1, T[i]} + \\beta_{2, T[i]} \\cdot \\log\\left(x^*_i\\right)\\\\\n\\log\\left(x_i^*\\right) &\\sim \\mathrm{Normal}\\left(\\mu_x, \\sigma^2_x\\right) \\\\\n\\mu_x &= \\alpha_{T[i]} \\\\\nx_i &= \\begin{cases}\nU, & x^*_i \\leq \\mathrm{LoD} \\\\\nx^*_i, & x^*_i > \\mathrm{LoD}\n\\end{cases} \\\\\nT[i] &= \\begin{cases}\n1, & \\text{individual } i \\text{ is in the placebo group} \\\\\n2, & \\text{individual } i \\text{ is in the vaccine group}\n\\end{cases}\n\\end{align*}\n$$\n\n* Because the effect of $x$ is group-dependent, parametrizing this in a frequentist\nway is really very difficult and there is not a director comparison to a\n`glm()`-type method.\n\n* We need to do a bit of set up with our data set.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnov_stan_dat <-\n\tnov_dat |>\n\tdplyr::mutate(\n\t\tt = treatment + 1,\n\t\tx = log(gii_pgm),\n\t\ty = protected,\n\t\t.keep = \"none\"\n\t) |>\n\tas.list()\nnov_stan_dat$N <- nrow(nov_dat)\nnov_stan_dat$k <- 2\n\n# Version of data where model doesn't implement censoring\nnov_stan_dat_naive <- nov_stan_dat\nnov_stan_dat_naive$x_l <- rep(0, times = nov_stan_dat$N)\n\n# Version of data where model DOES implement censoring\nnov_stan_dat_cens <- nov_stan_dat\nnov_stan_dat_cens$x_l <- rep(log(92), times = nov_stan_dat$N)\n\nstr(nov_stan_dat_cens, 1)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nList of 6\n $ t  : num [1:107] 2 2 2 2 2 2 1 1 2 1 ...\n $ x  : num [1:107] 7.36 6.34 6.85 6.35 7.2 ...\n $ y  : num [1:107] 0 1 1 1 0 1 1 1 1 1 ...\n $ N  : int 107\n $ k  : num 2\n $ x_l: num [1:107] 4.52 4.52 4.52 4.52 4.52 ...\n```\n\n\n:::\n:::\n\n\n\n* Let's take a look at the Stan code that implements the model.\n\n```{.stan include=\"stan/nov-censored-predictor.stan\"}\n```\n\n* We need to compile the stan code that implements this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Compile the stan model\nnov_mod_pth <- here::here(\"stan\", \"nov-censored-predictor.stan\")\nnov_stan_mod <- cmdstanr::cmdstan_model(nov_mod_pth, compile = FALSE)\nnov_stan_mod$compile(pedantic = TRUE, force_recompile = TRUE)\n```\n:::\n\n\n\n* Since there's no frequentist model to compare to, let's first run the model\nwithout a correction for censoring.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnov_stan_fit_naive <- nov_stan_mod$sample(\n\tnov_stan_dat_naive,\n\tseed = 923154,\n\tparallel_chains = 4,\n\titer_warmup = 1000,\n\titer_sampling = 1000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 1 finished in 2.3 seconds.\nChain 3 finished in 2.2 seconds.\nChain 4 finished in 2.2 seconds.\nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 finished in 2.4 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.3 seconds.\nTotal execution time: 2.6 seconds.\n```\n\n\n:::\n:::\n\n\n\nWe'll take a look at the model estimates.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnov_stan_fit_naive\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   variable   mean median   sd  mad     q5    q95 rhat ess_bulk ess_tail\n lp__       -95.66 -95.38 1.85 1.77 -99.11 -93.25 1.00     1718     2533\n alpha_1[1]   4.89   4.89 0.10 0.10   4.73   5.05 1.00     4370     2649\n alpha_1[2]   6.65   6.65 0.10 0.10   6.49   6.80 1.00     3853     2518\n sigma_x      0.70   0.69 0.05 0.05   0.62   0.78 1.00     4719     2881\n beta_1[1]   -3.68  -3.68 1.42 1.42  -5.99  -1.40 1.00     2307     2468\n beta_1[2]    0.32   0.37 1.63 1.62  -2.34   3.00 1.00     1850     2124\n beta_2[1]    0.68   0.68 0.29 0.30   0.21   1.17 1.00     2323     2470\n beta_2[2]   -0.07  -0.07 0.25 0.25  -0.47   0.34 1.00     1865     2219\n```\n\n\n:::\n:::\n\n\n\n* Now we can run the model with the censoring correction.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnov_stan_fit_cens <- nov_stan_mod$sample(\n\tnov_stan_dat_cens,\n\tseed = 923154,\n\tparallel_chains = 4,\n\titer_warmup = 1000,\n\titer_sampling = 1000\n)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRunning MCMC with 4 parallel chains...\n\nChain 1 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 2 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 3 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 4 Iteration:    1 / 2000 [  0%]  (Warmup) \nChain 1 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 1 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 2 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 3 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 3 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 4 Iteration:  100 / 2000 [  5%]  (Warmup) \nChain 4 Iteration:  200 / 2000 [ 10%]  (Warmup) \nChain 1 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 2 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 2 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 3 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 3 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 4 Iteration:  300 / 2000 [ 15%]  (Warmup) \nChain 4 Iteration:  400 / 2000 [ 20%]  (Warmup) \nChain 1 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 4 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 1 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  500 / 2000 [ 25%]  (Warmup) \nChain 3 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 4 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 1 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 2 Iteration:  600 / 2000 [ 30%]  (Warmup) \nChain 2 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 3 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 4 Iteration:  700 / 2000 [ 35%]  (Warmup) \nChain 1 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 2 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 3 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 4 Iteration:  800 / 2000 [ 40%]  (Warmup) \nChain 1 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 2 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 3 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 4 Iteration:  900 / 2000 [ 45%]  (Warmup) \nChain 1 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 1 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 2 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 2 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 3 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 3 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 4 Iteration: 1000 / 2000 [ 50%]  (Warmup) \nChain 4 Iteration: 1001 / 2000 [ 50%]  (Sampling) \nChain 1 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 2 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 3 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 4 Iteration: 1100 / 2000 [ 55%]  (Sampling) \nChain 1 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 2 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 3 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 4 Iteration: 1200 / 2000 [ 60%]  (Sampling) \nChain 1 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 2 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 3 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 4 Iteration: 1300 / 2000 [ 65%]  (Sampling) \nChain 1 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 2 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 3 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 4 Iteration: 1400 / 2000 [ 70%]  (Sampling) \nChain 1 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 2 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 3 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 4 Iteration: 1500 / 2000 [ 75%]  (Sampling) \nChain 1 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 3 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 4 Iteration: 1600 / 2000 [ 80%]  (Sampling) \nChain 1 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 2 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 2 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 3 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 4 Iteration: 1700 / 2000 [ 85%]  (Sampling) \nChain 1 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 2 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 4 Iteration: 1800 / 2000 [ 90%]  (Sampling) \nChain 1 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 2 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 3 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 1 finished in 2.4 seconds.\nChain 2 finished in 2.4 seconds.\nChain 3 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 Iteration: 1900 / 2000 [ 95%]  (Sampling) \nChain 3 finished in 2.5 seconds.\nChain 4 Iteration: 2000 / 2000 [100%]  (Sampling) \nChain 4 finished in 2.6 seconds.\n\nAll 4 chains finished successfully.\nMean chain execution time: 2.5 seconds.\nTotal execution time: 2.8 seconds.\n```\n\n\n:::\n:::\n\n\n\nAnd we can look at the estimates with the censoring correction -- notably our\neffect of the treatment estimates doesn't seem to change, but some of the\nothers change a bit.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nnov_stan_fit_cens\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n   variable    mean  median   sd  mad      q5     q95 rhat ess_bulk ess_tail\n lp__       -133.19 -132.83 1.87 1.69 -136.66 -130.78 1.00     1760     2390\n alpha_1[1]    4.35    4.36 0.16 0.15    4.08    4.60 1.00     3204     2577\n alpha_1[2]    6.64    6.64 0.13 0.13    6.43    6.85 1.00     3988     2836\n sigma_x       0.94    0.94 0.09 0.08    0.81    1.09 1.00     3208     2433\n beta_1[1]    -3.61   -3.59 1.51 1.57   -6.13   -1.17 1.00     2410     2219\n beta_1[2]     0.31    0.28 1.65 1.67   -2.38    3.05 1.00     2006     1695\n beta_2[1]     0.67    0.67 0.31 0.32    0.17    1.20 1.00     2460     2359\n beta_2[2]    -0.06   -0.06 0.25 0.25   -0.48    0.34 1.00     1890     1845\n```\n\n\n:::\n:::\n\n\n\n## Math on censored variables\n\nFor HAI score and other accepted surrogate endpoints in clinical trials,\nwe would like to analyze the change from baseline directly as the outcome of\nour model. For HAI, this usually called the fold change on the natural scale or\nthe titer increase on the log scale. We can compute this as follows.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nhai_dat$titer_increase <- hai_dat$posttiter - hai_dat$pretiter\nhai_dat$fold_change <- 2 ^ hai_dat$titer_increase\n```\n:::\n\n\n\nNotably, these two values both involve doing math on a censored variable, so\nit is not immediately obvious how to deal with censoring in these outcomes.\nFor the fold change, we consider the following cases.\n\n|  Pre-titer  |  Post-titer | Censored? |                     Fold change (lower bound)                     | Fold change (upper bound)                                      |\n|:-----------:|:-----------:|-----------|:-----------------------------------------------------------------:|----------------------------------------------------------------|\n| $$<10$$     | $$<10$$     | Yes       | $$\\lim_{b\\to 0} \\frac{\\text{post}}{b} = 0$$                       | $$\\lim_{a\\to 0} \\frac{a}{\\text{pre}} = \\infty$$                |\n| $$<10$$     | $$\\geq 10$$ | Yes       | $$\\lim_{b\\to 10} \\frac{\\text{post}}{b} = \\frac{\\text{post}}{10}$$ | $$\\lim_{b\\to 0} \\frac{\\text{post}}{b} = \\infty$$               |\n| $$\\geq 10$$ | $$<10$$     | Yes       | $$\\lim_{a\\to 0}\\frac{a}{\\text{pre}} = 0$$                         | $$\\lim_{a\\to 10}\\frac{a}{\\text{pre}} = \\frac{10}{\\text{pre}}$$ |\n| $$\\geq 10$$ | $$\\geq 10$$ | No        | $$\\frac{\\text{post}}{\\text{pre}}$$                                | $$\\frac{\\text{post}}{\\text{pre}}$$                             |\n\nTaking $\\lim_{a\\to 0} \\log(a) = -\\infty$ and $\\lim_{a\\to \\infty} \\log(a) = \\infty$, we can get the equivalent bounds for the titer increase. We can then implement models with the fold change or titer increase as the outcome. However, note that\nthis reduces our effective sample size for the process of estimation, because any values where both the pre-titer and post-titer are censored span the entire domain,\nand thus contribute nothing to the likelihood. This results as a consequence of\nthe indeterminate form $\\frac{0}{0}$ appearing in the limit.\n\nModeling the post-titer and pre-titer separately or jointly rather than modeling\nthe change will better account for censoring without the issue of this indeterminate form.\n\n<!-- END OF FILE -->\n",
    "supporting": [
      "main_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-after-body": [
        "\n<script>\n  // htmlwidgets need to know to resize themselves when slides are shown/hidden.\n  // Fire the \"slideenter\" event (handled by htmlwidgets.js) when the current\n  // slide changes (different for each slide format).\n  (function () {\n    // dispatch for htmlwidgets\n    function fireSlideEnter() {\n      const event = window.document.createEvent(\"Event\");\n      event.initEvent(\"slideenter\", true, true);\n      window.document.dispatchEvent(event);\n    }\n\n    function fireSlideChanged(previousSlide, currentSlide) {\n      fireSlideEnter();\n\n      // dispatch for shiny\n      if (window.jQuery) {\n        if (previousSlide) {\n          window.jQuery(previousSlide).trigger(\"hidden\");\n        }\n        if (currentSlide) {\n          window.jQuery(currentSlide).trigger(\"shown\");\n        }\n      }\n    }\n\n    // hookup for slidy\n    if (window.w3c_slidy) {\n      window.w3c_slidy.add_observer(function (slide_num) {\n        // slide_num starts at position 1\n        fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);\n      });\n    }\n\n  })();\n</script>\n\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}